{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ptb_cnn_0991.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaimetcf/ptb_cnn/blob/master/ptb_cnn_0991.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "To_ukPFJGi35",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from numpy.random import sample\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split, validation_curve\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "from tensorflow.python.keras.utils.np_utils import to_categorical\n",
        "from tensorflow.python.keras import initializers\n",
        "from tensorflow.python.keras import optimizers\n",
        "from tensorflow.python.keras import layers\n",
        "from tensorflow.python.keras import initializers\n",
        "from tensorflow.python.keras import regularizers\n",
        "from tensorflow.python.keras import models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTJ9ZUurG1i9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "f2f5f574-ce4b-436d-99d2-0667c8ba3e91"
      },
      "source": [
        "df_abnormal = pd.read_csv(\"ptbdb_abnormal.csv\", header=None)\n",
        "df_normal = pd.read_csv(\"ptbdb_normal.csv\", header=None)\n",
        "\n",
        "# Stacks df_abnormal and df_normal\n",
        "df = pd.concat([df_abnormal, df_normal], axis=0)\n",
        "print(\"\\ndf class counts\")\n",
        "print( df.iloc[:,187].value_counts() )\n",
        "print(\"df.shape => \", df.shape )"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "df class counts\n",
            "1.0    10506\n",
            "0.0     4046\n",
            "Name: 187, dtype: int64\n",
            "df.shape =>  (14552, 188)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xh5mewxZId1w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "f8202208-17a8-478f-e7c9-4343c8c7d0f8"
      },
      "source": [
        "X = df.iloc[:,0:-1]\n",
        "y = df.iloc[:,-1]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,train_size=0.80, shuffle=True, random_state=1)\n",
        "\n",
        "print(\"X_train.shape => \", X_train.shape )\n",
        "print(\"y_train.shape => \", y_train.shape )\n",
        "print(\"X_test.shape =>  \", X_test.shape )\n",
        "print(\"y_test.shape =>  \", y_test.shape )\n",
        "\n",
        "# Prepares data for CNN ##############################################################################\n",
        "\n",
        "# Standardizes data\n",
        "sc = StandardScaler(with_mean=True, with_std=True)\n",
        "sc.fit(X_train)\n",
        "x_train = sc.transform(X_train)\n",
        "x_test = sc.transform(X_test)\n",
        "\n",
        "# Reshapes arrays for cnn\n",
        "n_steps = x_train.shape[1]\n",
        "x_train = x_train.reshape(-1,n_steps,1)\n",
        "x_test = x_test.reshape(-1,n_steps,1)\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "print(\"x_train.shape => \", x_train.shape)\n",
        "print(\"x_test.shape  => \", x_test.shape)\n",
        "print(\"y_train.shape => \", y_train.shape)\n",
        "print(\"y_test.shape  => \", y_test.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train.shape =>  (11641, 187)\n",
            "y_train.shape =>  (11641,)\n",
            "X_test.shape =>   (2911, 187)\n",
            "y_test.shape =>   (2911,)\n",
            "x_train.shape =>  (11641, 187, 1)\n",
            "x_test.shape  =>  (2911, 187, 1)\n",
            "y_train.shape =>  (11641, 2)\n",
            "y_test.shape  =>  (2911, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3IDTWkiIhQl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "outputId": "91a7d838-2980-4f7b-e8f9-fd35bdd9a932"
      },
      "source": [
        "init = initializers.RandomUniform(minval=-0.5, maxval=0.5, seed=1)\n",
        "input = layers.Input(shape=(n_steps,1)) \n",
        "conv1 = layers.convolutional.Conv1D( filters=32, kernel_size=(5), activation='relu', \n",
        "                                     #activity_regularizer=regularizers.l2(0.01), \n",
        "                                     use_bias=True,bias_initializer=init, \n",
        "                                     kernel_initializer=init )(input) \n",
        "conv2 = layers.convolutional.Conv1D( filters=32, kernel_size=(5), activation='relu', \n",
        "                                     #activity_regularizer=regularizers.l2(0.01), \n",
        "                                     use_bias=True,bias_initializer=init, \n",
        "                                     kernel_initializer=init )(conv1) \n",
        "pool1 = layers.MaxPooling1D(pool_size=3)(conv2)\n",
        "\n",
        "conv3 = layers.convolutional.Conv1D( filters=32, kernel_size=(5), activation='relu', \n",
        "                                     #activity_regularizer=regularizers.l2(0.01), \n",
        "                                     use_bias=True,bias_initializer=init, \n",
        "                                     kernel_initializer=init )(pool1) \n",
        "conv4 = layers.convolutional.Conv1D( filters=32, kernel_size=(5), activation='relu', \n",
        "                                     #activity_regularizer=regularizers.l2(0.01), \n",
        "                                     use_bias=True,bias_initializer=init, \n",
        "                                     kernel_initializer=init )(conv3) \n",
        "pool2 = layers.MaxPooling1D(pool_size=3)(conv4)\n",
        "drop = layers.Dropout(0.5)(pool2)\n",
        "\n",
        "flat = layers.Flatten()(drop)\n",
        "\n",
        "dense1 = layers.Dense( 32,activation='relu', use_bias=True, bias_initializer=init, kernel_initializer=init )(flat) \n",
        "dense2 = layers.Dense( 2,activation='softmax', use_bias=True, bias_initializer=init, kernel_initializer=init )(dense1)\n",
        "\n",
        "model = models.Model(inputs=input, outputs=dense2) \n",
        "optim = optimizers.RMSprop(lr=0.0007) \n",
        "model.compile(optimizer=optim, loss='categorical_crossentropy', metrics=['accuracy'] )\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 187, 1)]          0         \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 183, 32)           192       \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 179, 32)           5152      \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 59, 32)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 55, 32)            5152      \n",
            "_________________________________________________________________\n",
            "conv1d_3 (Conv1D)            (None, 51, 32)            5152      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 17, 32)            0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 17, 32)            0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 544)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 32)                17440     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 66        \n",
            "=================================================================\n",
            "Total params: 33,154\n",
            "Trainable params: 33,154\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvj_bQ3lI6bt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3164080a-1077-44e9-b7d9-4de813f56fca"
      },
      "source": [
        "batch_size = 128\n",
        "\n",
        "history = model.fit(x_train, y_train, epochs=150, shuffle=False, batch_size=batch_size, verbose=1, validation_split=0.2)\n",
        "# max in epoch = 280\n",
        "\n",
        "# Prints model loss and validation accuracy vs epoch\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "print(\"Accuracy => \", model.evaluate(x_test, y_test))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 9312 samples, validate on 2329 samples\n",
            "Epoch 1/150\n",
            "9312/9312 [==============================] - 6s 650us/sample - loss: 5.5494 - acc: 0.6803 - val_loss: 0.8711 - val_acc: 0.7497\n",
            "Epoch 2/150\n",
            "9312/9312 [==============================] - 6s 601us/sample - loss: 1.0751 - acc: 0.7212 - val_loss: 0.4866 - val_acc: 0.7767\n",
            "Epoch 3/150\n",
            "9312/9312 [==============================] - 6s 604us/sample - loss: 0.5890 - acc: 0.7649 - val_loss: 0.3877 - val_acc: 0.8214\n",
            "Epoch 4/150\n",
            "9312/9312 [==============================] - 6s 619us/sample - loss: 0.4639 - acc: 0.7913 - val_loss: 0.3470 - val_acc: 0.8343\n",
            "Epoch 5/150\n",
            "9312/9312 [==============================] - 5s 587us/sample - loss: 0.4000 - acc: 0.8157 - val_loss: 0.3047 - val_acc: 0.8656\n",
            "Epoch 6/150\n",
            "9312/9312 [==============================] - 5s 577us/sample - loss: 0.3426 - acc: 0.8413 - val_loss: 0.2655 - val_acc: 0.8905\n",
            "Epoch 7/150\n",
            "9312/9312 [==============================] - 5s 577us/sample - loss: 0.3168 - acc: 0.8591 - val_loss: 0.2597 - val_acc: 0.8944\n",
            "Epoch 8/150\n",
            "9312/9312 [==============================] - 6s 592us/sample - loss: 0.2884 - acc: 0.8732 - val_loss: 0.2293 - val_acc: 0.9133\n",
            "Epoch 9/150\n",
            "9312/9312 [==============================] - 6s 595us/sample - loss: 0.2686 - acc: 0.8852 - val_loss: 0.2013 - val_acc: 0.9219\n",
            "Epoch 10/150\n",
            "9312/9312 [==============================] - 6s 595us/sample - loss: 0.2495 - acc: 0.8953 - val_loss: 0.1936 - val_acc: 0.9304\n",
            "Epoch 11/150\n",
            "9312/9312 [==============================] - 6s 603us/sample - loss: 0.2336 - acc: 0.9026 - val_loss: 0.1841 - val_acc: 0.9270\n",
            "Epoch 12/150\n",
            "9312/9312 [==============================] - 6s 595us/sample - loss: 0.2290 - acc: 0.9082 - val_loss: 0.1678 - val_acc: 0.9382\n",
            "Epoch 13/150\n",
            "9312/9312 [==============================] - 6s 597us/sample - loss: 0.2132 - acc: 0.9110 - val_loss: 0.1560 - val_acc: 0.9425\n",
            "Epoch 14/150\n",
            "9312/9312 [==============================] - 6s 596us/sample - loss: 0.2008 - acc: 0.9191 - val_loss: 0.1371 - val_acc: 0.9528\n",
            "Epoch 15/150\n",
            "9312/9312 [==============================] - 6s 607us/sample - loss: 0.1911 - acc: 0.9213 - val_loss: 0.1452 - val_acc: 0.9489\n",
            "Epoch 16/150\n",
            "9312/9312 [==============================] - 6s 608us/sample - loss: 0.1778 - acc: 0.9304 - val_loss: 0.1344 - val_acc: 0.9519\n",
            "Epoch 17/150\n",
            "9312/9312 [==============================] - 6s 610us/sample - loss: 0.1806 - acc: 0.9282 - val_loss: 0.1177 - val_acc: 0.9592\n",
            "Epoch 18/150\n",
            "9312/9312 [==============================] - 6s 610us/sample - loss: 0.1637 - acc: 0.9347 - val_loss: 0.1190 - val_acc: 0.9566\n",
            "Epoch 19/150\n",
            "9312/9312 [==============================] - 6s 595us/sample - loss: 0.1644 - acc: 0.9349 - val_loss: 0.1122 - val_acc: 0.9648\n",
            "Epoch 20/150\n",
            "9312/9312 [==============================] - 6s 600us/sample - loss: 0.1520 - acc: 0.9441 - val_loss: 0.1049 - val_acc: 0.9674\n",
            "Epoch 21/150\n",
            "9312/9312 [==============================] - 6s 598us/sample - loss: 0.1465 - acc: 0.9433 - val_loss: 0.0914 - val_acc: 0.9738\n",
            "Epoch 22/150\n",
            "9312/9312 [==============================] - 6s 615us/sample - loss: 0.1429 - acc: 0.9450 - val_loss: 0.0953 - val_acc: 0.9721\n",
            "Epoch 23/150\n",
            "9312/9312 [==============================] - 6s 599us/sample - loss: 0.1407 - acc: 0.9447 - val_loss: 0.0895 - val_acc: 0.9742\n",
            "Epoch 24/150\n",
            "9312/9312 [==============================] - 6s 612us/sample - loss: 0.1321 - acc: 0.9473 - val_loss: 0.0898 - val_acc: 0.9734\n",
            "Epoch 25/150\n",
            "9312/9312 [==============================] - 6s 615us/sample - loss: 0.1324 - acc: 0.9529 - val_loss: 0.0990 - val_acc: 0.9678\n",
            "Epoch 26/150\n",
            "9312/9312 [==============================] - 6s 603us/sample - loss: 0.1244 - acc: 0.9514 - val_loss: 0.1011 - val_acc: 0.9695\n",
            "Epoch 27/150\n",
            "9312/9312 [==============================] - 5s 590us/sample - loss: 0.1275 - acc: 0.9496 - val_loss: 0.0813 - val_acc: 0.9781\n",
            "Epoch 28/150\n",
            "9312/9312 [==============================] - 6s 605us/sample - loss: 0.1166 - acc: 0.9550 - val_loss: 0.0819 - val_acc: 0.9768\n",
            "Epoch 29/150\n",
            "9312/9312 [==============================] - 6s 597us/sample - loss: 0.1149 - acc: 0.9554 - val_loss: 0.0932 - val_acc: 0.9704\n",
            "Epoch 30/150\n",
            "9312/9312 [==============================] - 6s 607us/sample - loss: 0.1120 - acc: 0.9552 - val_loss: 0.0813 - val_acc: 0.9772\n",
            "Epoch 31/150\n",
            "9312/9312 [==============================] - 6s 599us/sample - loss: 0.1112 - acc: 0.9578 - val_loss: 0.1007 - val_acc: 0.9704\n",
            "Epoch 32/150\n",
            "9312/9312 [==============================] - 6s 602us/sample - loss: 0.1105 - acc: 0.9578 - val_loss: 0.0877 - val_acc: 0.9734\n",
            "Epoch 33/150\n",
            "9312/9312 [==============================] - 6s 596us/sample - loss: 0.0988 - acc: 0.9622 - val_loss: 0.0984 - val_acc: 0.9661\n",
            "Epoch 34/150\n",
            "9312/9312 [==============================] - 6s 593us/sample - loss: 0.0987 - acc: 0.9638 - val_loss: 0.0687 - val_acc: 0.9811\n",
            "Epoch 35/150\n",
            "9312/9312 [==============================] - 6s 596us/sample - loss: 0.0972 - acc: 0.9621 - val_loss: 0.0921 - val_acc: 0.9695\n",
            "Epoch 36/150\n",
            "9312/9312 [==============================] - 6s 607us/sample - loss: 0.0856 - acc: 0.9685 - val_loss: 0.0798 - val_acc: 0.9751\n",
            "Epoch 37/150\n",
            "9312/9312 [==============================] - 6s 605us/sample - loss: 0.0917 - acc: 0.9646 - val_loss: 0.0990 - val_acc: 0.9652\n",
            "Epoch 38/150\n",
            "9312/9312 [==============================] - 5s 583us/sample - loss: 0.0897 - acc: 0.9686 - val_loss: 0.0693 - val_acc: 0.9790\n",
            "Epoch 39/150\n",
            "9312/9312 [==============================] - 5s 590us/sample - loss: 0.0846 - acc: 0.9679 - val_loss: 0.0651 - val_acc: 0.9828\n",
            "Epoch 40/150\n",
            "9312/9312 [==============================] - 6s 595us/sample - loss: 0.0836 - acc: 0.9680 - val_loss: 0.0773 - val_acc: 0.9772\n",
            "Epoch 41/150\n",
            "9312/9312 [==============================] - 5s 586us/sample - loss: 0.0814 - acc: 0.9685 - val_loss: 0.0630 - val_acc: 0.9867\n",
            "Epoch 42/150\n",
            "9312/9312 [==============================] - 6s 614us/sample - loss: 0.0849 - acc: 0.9674 - val_loss: 0.0637 - val_acc: 0.9798\n",
            "Epoch 43/150\n",
            "9312/9312 [==============================] - 6s 593us/sample - loss: 0.0764 - acc: 0.9741 - val_loss: 0.0914 - val_acc: 0.9738\n",
            "Epoch 44/150\n",
            "9312/9312 [==============================] - 6s 606us/sample - loss: 0.0819 - acc: 0.9676 - val_loss: 0.0668 - val_acc: 0.9820\n",
            "Epoch 45/150\n",
            "9312/9312 [==============================] - 6s 601us/sample - loss: 0.0712 - acc: 0.9737 - val_loss: 0.0641 - val_acc: 0.9824\n",
            "Epoch 46/150\n",
            "9312/9312 [==============================] - 5s 583us/sample - loss: 0.0807 - acc: 0.9712 - val_loss: 0.0600 - val_acc: 0.9837\n",
            "Epoch 47/150\n",
            "9312/9312 [==============================] - 6s 598us/sample - loss: 0.0744 - acc: 0.9733 - val_loss: 0.0669 - val_acc: 0.9824\n",
            "Epoch 48/150\n",
            "9312/9312 [==============================] - 6s 596us/sample - loss: 0.0706 - acc: 0.9743 - val_loss: 0.0584 - val_acc: 0.9841\n",
            "Epoch 49/150\n",
            "9312/9312 [==============================] - 6s 594us/sample - loss: 0.0705 - acc: 0.9730 - val_loss: 0.0550 - val_acc: 0.9888\n",
            "Epoch 50/150\n",
            "9312/9312 [==============================] - 5s 578us/sample - loss: 0.0746 - acc: 0.9720 - val_loss: 0.0570 - val_acc: 0.9863\n",
            "Epoch 51/150\n",
            "9312/9312 [==============================] - 5s 582us/sample - loss: 0.0676 - acc: 0.9730 - val_loss: 0.0750 - val_acc: 0.9807\n",
            "Epoch 52/150\n",
            "9312/9312 [==============================] - 6s 593us/sample - loss: 0.0660 - acc: 0.9766 - val_loss: 0.0907 - val_acc: 0.9734\n",
            "Epoch 53/150\n",
            "9312/9312 [==============================] - 5s 562us/sample - loss: 0.0721 - acc: 0.9736 - val_loss: 0.0646 - val_acc: 0.9833\n",
            "Epoch 54/150\n",
            "9312/9312 [==============================] - 5s 565us/sample - loss: 0.0594 - acc: 0.9771 - val_loss: 0.0581 - val_acc: 0.9875\n",
            "Epoch 55/150\n",
            "9312/9312 [==============================] - 5s 587us/sample - loss: 0.0609 - acc: 0.9773 - val_loss: 0.0989 - val_acc: 0.9717\n",
            "Epoch 56/150\n",
            "9312/9312 [==============================] - 6s 596us/sample - loss: 0.0646 - acc: 0.9764 - val_loss: 0.0634 - val_acc: 0.9875\n",
            "Epoch 57/150\n",
            "9312/9312 [==============================] - 5s 553us/sample - loss: 0.0616 - acc: 0.9754 - val_loss: 0.0694 - val_acc: 0.9867\n",
            "Epoch 58/150\n",
            "9312/9312 [==============================] - 5s 562us/sample - loss: 0.0550 - acc: 0.9785 - val_loss: 0.0956 - val_acc: 0.9742\n",
            "Epoch 59/150\n",
            "9312/9312 [==============================] - 5s 579us/sample - loss: 0.0566 - acc: 0.9813 - val_loss: 0.0581 - val_acc: 0.9897\n",
            "Epoch 60/150\n",
            "9312/9312 [==============================] - 5s 577us/sample - loss: 0.0526 - acc: 0.9801 - val_loss: 0.0636 - val_acc: 0.9833\n",
            "Epoch 61/150\n",
            "9312/9312 [==============================] - 5s 565us/sample - loss: 0.0597 - acc: 0.9791 - val_loss: 0.0631 - val_acc: 0.9845\n",
            "Epoch 62/150\n",
            "9312/9312 [==============================] - 5s 561us/sample - loss: 0.0598 - acc: 0.9788 - val_loss: 0.0568 - val_acc: 0.9875\n",
            "Epoch 63/150\n",
            "9312/9312 [==============================] - 5s 561us/sample - loss: 0.0589 - acc: 0.9790 - val_loss: 0.0552 - val_acc: 0.9897\n",
            "Epoch 64/150\n",
            "9312/9312 [==============================] - 5s 582us/sample - loss: 0.0543 - acc: 0.9799 - val_loss: 0.0536 - val_acc: 0.9884\n",
            "Epoch 65/150\n",
            "9312/9312 [==============================] - 5s 548us/sample - loss: 0.0506 - acc: 0.9800 - val_loss: 0.0828 - val_acc: 0.9751\n",
            "Epoch 66/150\n",
            "9312/9312 [==============================] - 5s 543us/sample - loss: 0.0507 - acc: 0.9817 - val_loss: 0.0708 - val_acc: 0.9867\n",
            "Epoch 67/150\n",
            "9312/9312 [==============================] - 5s 581us/sample - loss: 0.0511 - acc: 0.9823 - val_loss: 0.0787 - val_acc: 0.9850\n",
            "Epoch 68/150\n",
            "9312/9312 [==============================] - 5s 548us/sample - loss: 0.0571 - acc: 0.9813 - val_loss: 0.0677 - val_acc: 0.9897\n",
            "Epoch 69/150\n",
            "9312/9312 [==============================] - 5s 575us/sample - loss: 0.0547 - acc: 0.9799 - val_loss: 0.0792 - val_acc: 0.9760\n",
            "Epoch 70/150\n",
            "9312/9312 [==============================] - 5s 584us/sample - loss: 0.0507 - acc: 0.9808 - val_loss: 0.0575 - val_acc: 0.9884\n",
            "Epoch 71/150\n",
            "9312/9312 [==============================] - 5s 550us/sample - loss: 0.0526 - acc: 0.9813 - val_loss: 0.0620 - val_acc: 0.9863\n",
            "Epoch 72/150\n",
            "9312/9312 [==============================] - 5s 557us/sample - loss: 0.0511 - acc: 0.9810 - val_loss: 0.0695 - val_acc: 0.9824\n",
            "Epoch 73/150\n",
            "9312/9312 [==============================] - 5s 553us/sample - loss: 0.0523 - acc: 0.9798 - val_loss: 0.0519 - val_acc: 0.9914\n",
            "Epoch 74/150\n",
            "9312/9312 [==============================] - 5s 582us/sample - loss: 0.0464 - acc: 0.9832 - val_loss: 0.0537 - val_acc: 0.9893\n",
            "Epoch 75/150\n",
            "9312/9312 [==============================] - 6s 594us/sample - loss: 0.0518 - acc: 0.9801 - val_loss: 0.0623 - val_acc: 0.9871\n",
            "Epoch 76/150\n",
            "9312/9312 [==============================] - 5s 559us/sample - loss: 0.0459 - acc: 0.9813 - val_loss: 0.0596 - val_acc: 0.9884\n",
            "Epoch 77/150\n",
            "9312/9312 [==============================] - 5s 544us/sample - loss: 0.0479 - acc: 0.9830 - val_loss: 0.0685 - val_acc: 0.9863\n",
            "Epoch 78/150\n",
            "9312/9312 [==============================] - 5s 573us/sample - loss: 0.0454 - acc: 0.9823 - val_loss: 0.0651 - val_acc: 0.9880\n",
            "Epoch 79/150\n",
            "9312/9312 [==============================] - 5s 548us/sample - loss: 0.0476 - acc: 0.9842 - val_loss: 0.0647 - val_acc: 0.9871\n",
            "Epoch 80/150\n",
            "9312/9312 [==============================] - 5s 546us/sample - loss: 0.0408 - acc: 0.9846 - val_loss: 0.0628 - val_acc: 0.9918\n",
            "Epoch 81/150\n",
            "9312/9312 [==============================] - 5s 575us/sample - loss: 0.0418 - acc: 0.9856 - val_loss: 0.0601 - val_acc: 0.9867\n",
            "Epoch 82/150\n",
            "9312/9312 [==============================] - 5s 570us/sample - loss: 0.0449 - acc: 0.9853 - val_loss: 0.0662 - val_acc: 0.9880\n",
            "Epoch 83/150\n",
            "9312/9312 [==============================] - 5s 569us/sample - loss: 0.0428 - acc: 0.9842 - val_loss: 0.0607 - val_acc: 0.9901\n",
            "Epoch 84/150\n",
            "9312/9312 [==============================] - 5s 580us/sample - loss: 0.0405 - acc: 0.9851 - val_loss: 0.0615 - val_acc: 0.9888\n",
            "Epoch 85/150\n",
            "9312/9312 [==============================] - 5s 568us/sample - loss: 0.0445 - acc: 0.9832 - val_loss: 0.0536 - val_acc: 0.9914\n",
            "Epoch 86/150\n",
            "9312/9312 [==============================] - 5s 576us/sample - loss: 0.0392 - acc: 0.9868 - val_loss: 0.0581 - val_acc: 0.9901\n",
            "Epoch 87/150\n",
            "9312/9312 [==============================] - 5s 584us/sample - loss: 0.0427 - acc: 0.9842 - val_loss: 0.0618 - val_acc: 0.9884\n",
            "Epoch 88/150\n",
            "9312/9312 [==============================] - 5s 580us/sample - loss: 0.0356 - acc: 0.9878 - val_loss: 0.0694 - val_acc: 0.9867\n",
            "Epoch 89/150\n",
            "9312/9312 [==============================] - 5s 578us/sample - loss: 0.0451 - acc: 0.9838 - val_loss: 0.0602 - val_acc: 0.9880\n",
            "Epoch 90/150\n",
            "9312/9312 [==============================] - 5s 576us/sample - loss: 0.0416 - acc: 0.9855 - val_loss: 0.0743 - val_acc: 0.9850\n",
            "Epoch 91/150\n",
            "9312/9312 [==============================] - 5s 564us/sample - loss: 0.0355 - acc: 0.9863 - val_loss: 0.0549 - val_acc: 0.9901\n",
            "Epoch 92/150\n",
            "9312/9312 [==============================] - 5s 578us/sample - loss: 0.0451 - acc: 0.9839 - val_loss: 0.0810 - val_acc: 0.9815\n",
            "Epoch 93/150\n",
            "9312/9312 [==============================] - 5s 569us/sample - loss: 0.0380 - acc: 0.9865 - val_loss: 0.0582 - val_acc: 0.9880\n",
            "Epoch 94/150\n",
            "9312/9312 [==============================] - 5s 575us/sample - loss: 0.0358 - acc: 0.9859 - val_loss: 0.0605 - val_acc: 0.9906\n",
            "Epoch 95/150\n",
            "9312/9312 [==============================] - 5s 571us/sample - loss: 0.0371 - acc: 0.9880 - val_loss: 0.0619 - val_acc: 0.9897\n",
            "Epoch 96/150\n",
            "9312/9312 [==============================] - 5s 561us/sample - loss: 0.0415 - acc: 0.9850 - val_loss: 0.0988 - val_acc: 0.9790\n",
            "Epoch 97/150\n",
            "9312/9312 [==============================] - 6s 600us/sample - loss: 0.0339 - acc: 0.9877 - val_loss: 0.0518 - val_acc: 0.9906\n",
            "Epoch 98/150\n",
            "9312/9312 [==============================] - 5s 587us/sample - loss: 0.0418 - acc: 0.9861 - val_loss: 0.0626 - val_acc: 0.9884\n",
            "Epoch 99/150\n",
            "9312/9312 [==============================] - 5s 567us/sample - loss: 0.0355 - acc: 0.9872 - val_loss: 0.0702 - val_acc: 0.9893\n",
            "Epoch 100/150\n",
            "9312/9312 [==============================] - 5s 570us/sample - loss: 0.0370 - acc: 0.9873 - val_loss: 0.0973 - val_acc: 0.9768\n",
            "Epoch 101/150\n",
            "9312/9312 [==============================] - 5s 582us/sample - loss: 0.0359 - acc: 0.9873 - val_loss: 0.0580 - val_acc: 0.9906\n",
            "Epoch 102/150\n",
            "9312/9312 [==============================] - 5s 561us/sample - loss: 0.0428 - acc: 0.9865 - val_loss: 0.0544 - val_acc: 0.9897\n",
            "Epoch 103/150\n",
            "9312/9312 [==============================] - 5s 558us/sample - loss: 0.0337 - acc: 0.9892 - val_loss: 0.0865 - val_acc: 0.9798\n",
            "Epoch 104/150\n",
            "9312/9312 [==============================] - 6s 608us/sample - loss: 0.0358 - acc: 0.9881 - val_loss: 0.0576 - val_acc: 0.9884\n",
            "Epoch 105/150\n",
            "9312/9312 [==============================] - 5s 548us/sample - loss: 0.0411 - acc: 0.9850 - val_loss: 0.0879 - val_acc: 0.9811\n",
            "Epoch 106/150\n",
            "9312/9312 [==============================] - 5s 557us/sample - loss: 0.0345 - acc: 0.9883 - val_loss: 0.1299 - val_acc: 0.9717\n",
            "Epoch 107/150\n",
            "9312/9312 [==============================] - 5s 552us/sample - loss: 0.0380 - acc: 0.9872 - val_loss: 0.0593 - val_acc: 0.9910\n",
            "Epoch 108/150\n",
            "9312/9312 [==============================] - 5s 570us/sample - loss: 0.0324 - acc: 0.9882 - val_loss: 0.0588 - val_acc: 0.9893\n",
            "Epoch 109/150\n",
            "9312/9312 [==============================] - 5s 587us/sample - loss: 0.0287 - acc: 0.9893 - val_loss: 0.0846 - val_acc: 0.9811\n",
            "Epoch 110/150\n",
            "9312/9312 [==============================] - 5s 560us/sample - loss: 0.0382 - acc: 0.9868 - val_loss: 0.0843 - val_acc: 0.9850\n",
            "Epoch 111/150\n",
            "9312/9312 [==============================] - 5s 589us/sample - loss: 0.0346 - acc: 0.9880 - val_loss: 0.0737 - val_acc: 0.9884\n",
            "Epoch 112/150\n",
            "9312/9312 [==============================] - 6s 601us/sample - loss: 0.0369 - acc: 0.9870 - val_loss: 0.0701 - val_acc: 0.9871\n",
            "Epoch 113/150\n",
            "9312/9312 [==============================] - 6s 611us/sample - loss: 0.0319 - acc: 0.9877 - val_loss: 0.0656 - val_acc: 0.9884\n",
            "Epoch 114/150\n",
            "9312/9312 [==============================] - 5s 590us/sample - loss: 0.0323 - acc: 0.9899 - val_loss: 0.0608 - val_acc: 0.9880\n",
            "Epoch 115/150\n",
            "9312/9312 [==============================] - 6s 599us/sample - loss: 0.0315 - acc: 0.9896 - val_loss: 0.0834 - val_acc: 0.9828\n",
            "Epoch 116/150\n",
            "9312/9312 [==============================] - 6s 604us/sample - loss: 0.0362 - acc: 0.9867 - val_loss: 0.0650 - val_acc: 0.9893\n",
            "Epoch 117/150\n",
            "9312/9312 [==============================] - 6s 600us/sample - loss: 0.0324 - acc: 0.9885 - val_loss: 0.0768 - val_acc: 0.9871\n",
            "Epoch 118/150\n",
            "9312/9312 [==============================] - 6s 614us/sample - loss: 0.0310 - acc: 0.9895 - val_loss: 0.0735 - val_acc: 0.9897\n",
            "Epoch 119/150\n",
            "9312/9312 [==============================] - 6s 603us/sample - loss: 0.0326 - acc: 0.9887 - val_loss: 0.0712 - val_acc: 0.9863\n",
            "Epoch 120/150\n",
            "9312/9312 [==============================] - 6s 607us/sample - loss: 0.0269 - acc: 0.9901 - val_loss: 0.0691 - val_acc: 0.9901\n",
            "Epoch 121/150\n",
            "9312/9312 [==============================] - 6s 592us/sample - loss: 0.0319 - acc: 0.9863 - val_loss: 0.1118 - val_acc: 0.9729\n",
            "Epoch 122/150\n",
            "9312/9312 [==============================] - 6s 598us/sample - loss: 0.0319 - acc: 0.9890 - val_loss: 0.0809 - val_acc: 0.9867\n",
            "Epoch 123/150\n",
            "9312/9312 [==============================] - 6s 616us/sample - loss: 0.0307 - acc: 0.9892 - val_loss: 0.0693 - val_acc: 0.9863\n",
            "Epoch 124/150\n",
            "9312/9312 [==============================] - 5s 584us/sample - loss: 0.0308 - acc: 0.9895 - val_loss: 0.0592 - val_acc: 0.9923\n",
            "Epoch 125/150\n",
            "9312/9312 [==============================] - 5s 588us/sample - loss: 0.0276 - acc: 0.9892 - val_loss: 0.0654 - val_acc: 0.9910\n",
            "Epoch 126/150\n",
            "9312/9312 [==============================] - 5s 590us/sample - loss: 0.0329 - acc: 0.9893 - val_loss: 0.0685 - val_acc: 0.9858\n",
            "Epoch 127/150\n",
            "9312/9312 [==============================] - 6s 599us/sample - loss: 0.0317 - acc: 0.9880 - val_loss: 0.0665 - val_acc: 0.9897\n",
            "Epoch 128/150\n",
            "9312/9312 [==============================] - 6s 601us/sample - loss: 0.0300 - acc: 0.9897 - val_loss: 0.0543 - val_acc: 0.9906\n",
            "Epoch 129/150\n",
            "9312/9312 [==============================] - 6s 612us/sample - loss: 0.0299 - acc: 0.9904 - val_loss: 0.0583 - val_acc: 0.9910\n",
            "Epoch 130/150\n",
            "9312/9312 [==============================] - 6s 598us/sample - loss: 0.0260 - acc: 0.9898 - val_loss: 0.0805 - val_acc: 0.9906\n",
            "Epoch 131/150\n",
            "9312/9312 [==============================] - 6s 609us/sample - loss: 0.0307 - acc: 0.9887 - val_loss: 0.0630 - val_acc: 0.9906\n",
            "Epoch 132/150\n",
            "9312/9312 [==============================] - 6s 614us/sample - loss: 0.0305 - acc: 0.9894 - val_loss: 0.0753 - val_acc: 0.9863\n",
            "Epoch 133/150\n",
            "9312/9312 [==============================] - 6s 598us/sample - loss: 0.0236 - acc: 0.9916 - val_loss: 0.0854 - val_acc: 0.9875\n",
            "Epoch 134/150\n",
            "9312/9312 [==============================] - 6s 616us/sample - loss: 0.0310 - acc: 0.9877 - val_loss: 0.0610 - val_acc: 0.9884\n",
            "Epoch 135/150\n",
            "9312/9312 [==============================] - 6s 600us/sample - loss: 0.0332 - acc: 0.9895 - val_loss: 0.0670 - val_acc: 0.9884\n",
            "Epoch 136/150\n",
            "9312/9312 [==============================] - 6s 613us/sample - loss: 0.0284 - acc: 0.9914 - val_loss: 0.0704 - val_acc: 0.9888\n",
            "Epoch 137/150\n",
            "9312/9312 [==============================] - 6s 609us/sample - loss: 0.0328 - acc: 0.9878 - val_loss: 0.0733 - val_acc: 0.9850\n",
            "Epoch 138/150\n",
            "9312/9312 [==============================] - 5s 586us/sample - loss: 0.0298 - acc: 0.9886 - val_loss: 0.0596 - val_acc: 0.9901\n",
            "Epoch 139/150\n",
            "9312/9312 [==============================] - 6s 605us/sample - loss: 0.0240 - acc: 0.9907 - val_loss: 0.0787 - val_acc: 0.9875\n",
            "Epoch 140/150\n",
            "9312/9312 [==============================] - 5s 588us/sample - loss: 0.0259 - acc: 0.9904 - val_loss: 0.0606 - val_acc: 0.9901\n",
            "Epoch 141/150\n",
            "9312/9312 [==============================] - 6s 605us/sample - loss: 0.0219 - acc: 0.9912 - val_loss: 0.0776 - val_acc: 0.9893\n",
            "Epoch 142/150\n",
            "9312/9312 [==============================] - 5s 582us/sample - loss: 0.0267 - acc: 0.9903 - val_loss: 0.0678 - val_acc: 0.9871\n",
            "Epoch 143/150\n",
            "9312/9312 [==============================] - 6s 609us/sample - loss: 0.0266 - acc: 0.9909 - val_loss: 0.0685 - val_acc: 0.9906\n",
            "Epoch 144/150\n",
            "9312/9312 [==============================] - 6s 603us/sample - loss: 0.0279 - acc: 0.9901 - val_loss: 0.0983 - val_acc: 0.9824\n",
            "Epoch 145/150\n",
            "9312/9312 [==============================] - 6s 606us/sample - loss: 0.0278 - acc: 0.9910 - val_loss: 0.0739 - val_acc: 0.9893\n",
            "Epoch 146/150\n",
            "9312/9312 [==============================] - 6s 607us/sample - loss: 0.0271 - acc: 0.9911 - val_loss: 0.0665 - val_acc: 0.9884\n",
            "Epoch 147/150\n",
            "9312/9312 [==============================] - 6s 618us/sample - loss: 0.0279 - acc: 0.9898 - val_loss: 0.0672 - val_acc: 0.9906\n",
            "Epoch 148/150\n",
            "9312/9312 [==============================] - 6s 609us/sample - loss: 0.0272 - acc: 0.9913 - val_loss: 0.0757 - val_acc: 0.9888\n",
            "Epoch 149/150\n",
            "9312/9312 [==============================] - 6s 606us/sample - loss: 0.0297 - acc: 0.9903 - val_loss: 0.0627 - val_acc: 0.9910\n",
            "Epoch 150/150\n",
            "9312/9312 [==============================] - 5s 587us/sample - loss: 0.0239 - acc: 0.9918 - val_loss: 0.0600 - val_acc: 0.9910\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIoQQQl1RLEIIIdQVxSKEEEJd/wK/m5TdHVA6aAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "2911/2911 [==============================] - 1s 236us/sample - loss: 0.0685 - acc: 0.9880\n",
            "Accuracy =>  [0.06847158022198029, 0.9879766]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}